{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Define The Problem\n",
    "\n",
    "Develop an algorithm to to predict the survival outcome of passengers on the Titanic.\n",
    "\n",
    "**Project Summary:** \n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this project, we are tasked to complete the analysis of what sorts of people were likely to survive. In particular, we will apply the tools of machine learning to predict which passengers survived the tragedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Gather the Data\n",
    "The dataset can be found at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare Data\n",
    "Because we were served the dataset, we don't need to worry about data wrangling, architecture, governance, and extraction. We only need to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Import Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output, call\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "import sklearn\n",
    "from IPython import display\n",
    "import IPython\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Matplotlib version:\", matplotlib.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"SciPy version:\", sp.__version__)\n",
    "print(\"IPython version:\", IPython.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "print(os.listdir('data'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Import Modeling Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common models\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# common model helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import feature_selection, model_selection, metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# configure visualziation defaults\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('darkgrid')\n",
    "mpl.rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Meet the Data\n",
    "Get to know the data.\n",
    "What are the feature names and datatypes?\n",
    "What are the target names and datatypes?\n",
    "\n",
    "Using the [Source Data Dictionary](https://www.kaggle.com/c/titanic/data), we can learn a bit about our Target and Features.\n",
    "\n",
    "1. The **Survived** variable is our outcome or dependent variable (Target). It is a binary nominal datatype of 1 for survived and 0 for did not survive. \n",
    "\n",
    "2. The **PassengerID** and **Ticket** variables are assumed to be random unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.\n",
    "\n",
    "3. The **Pclass** variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), representing 1 = upper class, 2 = middle class, and 3 = lower class.\n",
    "\n",
    "4. The **Name** variable is a nominal datatype. It could be used in feature engineering to derive the gender from title, family size from surname, and SES from titles like doctor or master. Since these variables already exist, we'll make use of it to see if title, like master, makes a difference.\n",
    "\n",
    "5. The **Sex** and **Embarked** variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.\n",
    "\n",
    "6. The **Age** and **Fare** variable are continuous quantitative datatypes.\n",
    "\n",
    "7. The **SibSp** represents number of related siblings/spouse aboard and **Parch** represents number of related parents/children aboard. Both are discrete quantitative datatypes. This can be used for feature engineering to create a family size and is alone variable.\n",
    "\n",
    "8. The **Cabin** variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred and SES from deck levels. However, since there are many null values, it does not add value and thus is excluded from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data will be broken into 3 parts: Train Set, Test Set, and Validation Set\n",
    "# the train file will be loaded into the data_raw dataframe, which will later be split into the Train Set and Test Set\n",
    "# the test file will be loaded into the data_val dataframe, which will be used as the Validation Set\n",
    "\n",
    "data_raw = pd.read_csv('data/train.csv')\n",
    "\n",
    "data_val = pd.read_csv('data/test.csv')\n",
    "\n",
    "# create a copy of data_raw to be transformed\n",
    "# deep=True by default. A deep copy will create a copy of the dataframe and all of its child objects, such as the data and the indeces.\n",
    "# deep=False will create a shallow copy, which will only copy the top level of the dataframe (its columns) and not its data\n",
    "data_1 = data_raw.copy(deep=True)\n",
    "\n",
    "datasets_to_clean = [data_1, data_val]\n",
    "\n",
    "# preview the data\n",
    "print(data_raw.info())\n",
    "data_raw.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Concatenate the train and test dataframes into one dataframe\n",
    "    \"\"\"\n",
    "    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n",
    "\n",
    "def divide_df(all_data):\n",
    "    \"\"\"\n",
    "    Divide the dataframe into the train and test dataframes\n",
    "    \"\"\"\n",
    "    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_all = concat_df(df_train, df_test)\n",
    "\n",
    "df_train.name = 'Training Set'\n",
    "df_test.name = 'Test Set'\n",
    "df_all.name = 'All Set'\n",
    "\n",
    "dfs = [df_train, df_test]\n",
    "\n",
    "print('Number of Training Examples: ', df_train.shape[0])\n",
    "print('Number of Test Examples: ', df_test.shape[0])\n",
    "print('Training X Shape: ', df_train.shape)\n",
    "print('Training Y Shape: ', df_train['Survived'].shape[0])\n",
    "print('Test X Shape: ', df_test.shape)\n",
    "print('Test Y Shape: ', df_test.shape[0])\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Cleaning the Data\n",
    "In this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n",
    "\n",
    "1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n",
    "\n",
    "2. **Completing:** There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. Subsequent model iterations may modify this decision to determine if it improves the modelâ€™s accuracy.\n",
    "\n",
    "3. **Creating:**  Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n",
    "\n",
    "4. **Converting:** Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Set, columns with null values:\\n', data_1.isnull().sum())\n",
    "print('-'*25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation Set, columns with null values:\\n', data_val.isnull().sum())\n",
    "print('-'*25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.1 Data Cleaning: Complete or Delete Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_to_clean:\n",
    "    # complete missing age with median\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace=True)\n",
    "\n",
    "    # complete embarked with mode\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "    # complete missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace=True)\n",
    "\n",
    "# delete the following features: Cabin, PassengerID, Ticket\n",
    "columns_to_drop = ['Cabin', 'PassengerId', 'Ticket']\n",
    "data_1.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data_1.isnull().sum())\n",
    "print('-'*25)\n",
    "print(data_val.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.2 Create: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_to_clean:\n",
    "    # discrete variables\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "    # initalize 'IsAlone' feature as 1\n",
    "    dataset['IsAlone'] = 1\n",
    "\n",
    "    # if FamilySize is greater than 1, then 'IsAlone' is 0\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n",
    "\n",
    "    # split title from name\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "\n",
    "    # continuous variable bins using qcut\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "\n",
    "    # create age bins\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.Title.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up rare titles\n",
    "title_frequency_threshold = 10\n",
    "titles_to_remove = (data_1.Title.value_counts() < title_frequency_threshold)\n",
    "\n",
    "# replace titles to remove with 'Misc'\n",
    "data_1['Title'] = data_1['Title'].apply(lambda x: 'Misc' if titles_to_remove.loc[x] == True else x)\n",
    "\n",
    "data_1.Title.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the data again\n",
    "data_1.info()\n",
    "data_1.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.info()\n",
    "data_val.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.3 Convert Formats\n",
    "We will convert categorical data to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for dataset in datasets_to_clean:\n",
    "    dataset['Sex_Code'] = label_encoder.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label_encoder.fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_Code'] = label_encoder.fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_Code'] = label_encoder.fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_Code'] = label_encoder.fit_transform(dataset['FareBin'])\n",
    "\n",
    "data_1.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define y variable AKA Target\n",
    "target = ['Survived']\n",
    "\n",
    "# define x variables AKA Features\n",
    "# pretty name/values for charts\n",
    "data_1_x = ['Sex', 'Pclass', 'Embarked', 'Title', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone']\n",
    "# coded for algorithm calculation\n",
    "data_1_x_calc = ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'SibSp', 'Parch', 'Age', 'Fare']\n",
    "\n",
    "data_1_xy = target + data_1_x\n",
    "print('Original X Y: ', data_1_xy, '\\n')\n",
    "\n",
    "# define x variables for original w/bin features to remove continuous variables\n",
    "data_1_x_bin = ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "data_1_xy_bin = target + data_1_x_bin\n",
    "print('Bin X Y: ', data_1_xy_bin, '\\n')\n",
    "\n",
    "# define x and y variables for dummy features original\n",
    "data_1_dummy = pd.get_dummies(data_1[data_1_x])\n",
    "data_1_x_dummy = data_1_dummy.columns.tolist()\n",
    "data_1_xy_dummy = target + data_1_x_dummy\n",
    "print('Dummy X Y: ', data_1_xy_dummy, '\\n')\n",
    "\n",
    "data_1_dummy.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.4 Double Check Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Set, columns with null values:\\n', data_1.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation Set, columns with null values:\\n', data_val.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Split Data_1 into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_x, test_1_x, train_1_y, test_1_y = model_selection.train_test_split(\n",
    "    data_1[data_1_x_calc], data_1[target], random_state=0)\n",
    "train_1_x_bin, test_1_x_bin, train_1_y_bin, test_1_y_bin = model_selection.train_test_split(\n",
    "    data_1[data_1_x_bin], data_1[target], random_state=0)\n",
    "train_1_x_dummy, test_1_x_dummy, train_1_y_dummy, test_1_y_dummy = model_selection.train_test_split(\n",
    "    data_1_dummy\n",
    "    [data_1_x_dummy],\n",
    "    data_1[target],\n",
    "    random_state=0)\n",
    "\n",
    "print('Data 1 Shape: ', data_1.shape)\n",
    "print('Train 1 X Shape: ', train_1_x.shape)\n",
    "print('Test 1 X Shape: ', test_1_x.shape)\n",
    "\n",
    "train_1_x_bin.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Perform Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Discrete Variable Correlcation by Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pivot table\n",
    "for x in data_1_x:\n",
    "    if data_1[x].dtype != 'float64':\n",
    "        print('Survival Correlation by: ', x)\n",
    "        print(data_1[[x, target[0]]].groupby([x], as_index=False).mean())\n",
    "        print('-'*25, '\\n')\n",
    "\n",
    "# using crosstab\n",
    "print(pd.crosstab(data_1['Title'], data_1[target[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph distribution of quantitative data\n",
    "plt.figure(figsize=[16, 12])\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.boxplot(x=data_1['Fare'], showmeans=True, meanline=True)\n",
    "plt.title('Fare Boxplot')\n",
    "plt.ylabel('Fare ($)')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.boxplot(data_1['Age'], showmeans=True, meanline=True)\n",
    "plt.title('Age Boxplot')\n",
    "plt.ylabel('Age (Years)')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.boxplot(data_1['FamilySize'], showmeans=True, meanline=True)\n",
    "plt.title('Family Size Boxplot')\n",
    "plt.ylabel('Family Size (#)')\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(x=[data_1[data_1['Survived'] == 1]['Fare'], data_1[data_1['Survived'] == 0]['Fare']],\n",
    "         stacked=True, color=['g', 'r'], label=['Survived', 'Dead'], bins=10)\n",
    "plt.title('Fare Histogram by Survival')\n",
    "plt.xlabel('Fare ($)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(x=[data_1[data_1['Survived'] == 1]['Age'], data_1[data_1['Survived'] == 0]['Age']],\n",
    "         stacked=True, color=['g', 'r'], label=['Survived', 'Dead'], bins=10)\n",
    "plt.title('Age Histogram by Survival')\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(x=[data_1[data_1['Survived'] == 1]['FamilySize'], data_1[data_1['Survived'] == 0]['FamilySize']],\n",
    "         stacked=True, color=['g', 'r'], label=['Survived', 'Dead'], bins=10)\n",
    "plt.title('Family Size Histogram by Survival')\n",
    "plt.xlabel('Family Size (#)')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn graphics for multi-variable comparison:\n",
    "\n",
    "# graph individual features by survival\n",
    "fig, saxis = plt.subplots(2, 3, figsize=(16, 12))\n",
    "\n",
    "sns.barplot(x='Embarked', y='Survived', data=data_1, ax=saxis[0, 0])\n",
    "sns.barplot(x='Pclass', y='Survived', order=[1, 2, 3], data=data_1, ax=saxis[0, 1])\n",
    "sns.barplot(x='IsAlone', y='Survived', order=[1, 0], data=data_1, ax=saxis[0, 2])\n",
    "\n",
    "sns.pointplot(x='FareBin', y='Survived',  data=data_1, ax=saxis[1, 0])\n",
    "sns.pointplot(x='AgeBin', y='Survived',  data=data_1, ax=saxis[1, 1])\n",
    "sns.pointplot(x='FamilySize', y='Survived', data=data_1, ax=saxis[1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph distribution of quantitative data: Pclass\n",
    "# we know class mattered in survival, so let's graph it against other features\n",
    "fig, (axis1, axis2, axis3) = plt.subplots(1, 3, figsize=(14, 12))\n",
    "\n",
    "sns.boxplot(x='Pclass', y='Fare', hue='Survived', data=data_1, ax=axis1)\n",
    "axis1.set_title('Class vs Fare Survival Comparison')\n",
    "\n",
    "sns.violinplot(x='Pclass', y='Age', hue='Survived', data=data_1, split=True, ax=axis2)\n",
    "axis2.set_title('Class vs Age Survival Comparison')\n",
    "\n",
    "sns.boxplot(x='Pclass', y='FamilySize', hue='Survived', data=data_1, ax=axis3)\n",
    "axis3.set_title('Class vs Family Size Survival Comparison')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph distribution of quantitative data: Sex\n",
    "# we know sex mattered in survival, so let's graph it against other features\n",
    "\n",
    "fig, qaxis = plt.subplots(nrows=1, ncols=3, figsize=[14, 12])\n",
    "\n",
    "sns.barplot(x='Sex', y='Survived', hue='Embarked', data=data_1, ax=qaxis[0])\n",
    "axis1.set_title('Sex vs Embarked Survival Comparison')\n",
    "\n",
    "sns.barplot(x='Sex', y='Survived', hue='Pclass', data=data_1, ax=qaxis[1])\n",
    "axis2.set_title('Sex vs Pclass Survival Comparison')\n",
    "\n",
    "sns.barplot(x='Sex', y='Survived', hue='IsAlone', data=data_1, ax=qaxis[2])\n",
    "axis3.set_title('Sex vs IsAlone Survival Comparison')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more side-by-side comparisons\n",
    "fig, (maxis1, maxis2) = plt.subplots(nrows=1, ncols=2, figsize=[14, 12])\n",
    "\n",
    "# how does family size factor with sex & survival compare\n",
    "sns.pointplot(\n",
    "    x='FamilySize',\n",
    "    y='Survived',\n",
    "    hue='Sex',\n",
    "    data=data_1,\n",
    "    palette={\n",
    "        'male': 'blue',\n",
    "        'female': 'pink'\n",
    "    },\n",
    "    markers=['*', 'o'],\n",
    "    linestyles=['-', '--'],\n",
    "    ax=maxis1\n",
    ")\n",
    "\n",
    "# how does class factor with sex & survival compare\n",
    "sns.pointplot(\n",
    "    x='Pclass',\n",
    "    y='Survived',\n",
    "    hue='Sex',\n",
    "    data=data_1,\n",
    "    palette={\n",
    "        'male': 'blue',\n",
    "        'female': 'pink'\n",
    "    },\n",
    "    markers=['*', 'o'],\n",
    "    linestyles=['-', '--'],\n",
    "    ax=maxis2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does embark port factor with class, sex, and survival outcome?\n",
    "embarked_facet_grid = sns.FacetGrid(data=data_1, col='Embarked')\n",
    "embarked_facet_grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette='deep')\n",
    "embarked_facet_grid.add_legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions of age of passengers who survived or did not survive\n",
    "age_facet_grid = sns.FacetGrid(data=data_1, hue='Survived', aspect=4)\n",
    "age_facet_grid.map(sns.kdeplot, 'Age', shade=True)\n",
    "age_facet_grid.set(xlim=(0, data_1['Age'].max()))\n",
    "age_facet_grid.add_legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram comparison of sex, class, and age by survival\n",
    "histogram_facet_grid = sns.FacetGrid(data=data_1, row='Sex', col='Pclass', hue='Survived')\n",
    "histogram_facet_grid.map(plt.hist, 'Age', alpha=0.75)\n",
    "histogram_facet_grid.add_legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plots of the entire dataset\n",
    "pair_plot = sns.pairplot(data=data_1, hue='Survived', palette='deep', size=1.2,\n",
    "                         diag_kind='kde', diag_kws=dict(shade=True), plot_kws=dict(s=10))\n",
    "pair_plot.set(xticklabels=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _, ax = plt.subplots(figsize=(10, 10))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    _ = sns.heatmap(\n",
    "        df.corr(),\n",
    "        cmap=colormap,\n",
    "        square=True,\n",
    "        cbar_kws={'shrink': .9},\n",
    "        ax=ax,\n",
    "        annot=True,\n",
    "        linewidths=0.1,\n",
    "        vmax=1.0,\n",
    "        linecolor='white',\n",
    "        annot_kws={'fontsize': 12}\n",
    "    )\n",
    "\n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "\n",
    "correlation_heatmap(data_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Machine Learning Algorith (MLA) Selection and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLAs = [\n",
    "    # ensemble methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    # Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "\n",
    "    # GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "\n",
    "    # Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "\n",
    "    # Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "\n",
    "    # SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "\n",
    "    # Trees\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "\n",
    "    # Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    # XGBoost\n",
    "    XGBClassifier()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset in cross-validation\n",
    "# run model 10 times with 60/30 split intentionally leaving out 10%\n",
    "cross_validation_split = model_selection.ShuffleSplit(n_splits=10, test_size=0.3, train_size=0.6, random_state=0)\n",
    "\n",
    "# create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train Accuracy Mean',\n",
    "               'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD']\n",
    "MLA_compare = pd.DataFrame(columns=MLA_columns)\n",
    "\n",
    "# create table to compare MLA predictions\n",
    "MLA_predict = data_1[target]\n",
    "\n",
    "# index through MLAs and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLAs:\n",
    "    # set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "\n",
    "    # score model with cross validation\n",
    "    cross_validation_results = model_selection.cross_validate(\n",
    "        alg,\n",
    "        data_1[data_1_x_bin],\n",
    "        data_1[target],\n",
    "        cv=cross_validation_split,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cross_validation_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cross_validation_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cross_validation_results['test_score'].mean()\n",
    "    # if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean,\n",
    "    # should statistically capture 99.7% of the subsets\n",
    "    # let's know the worst that can happen if we're really unlucky\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cross_validation_results['test_score'].std() * 3\n",
    "\n",
    "    # save MLA predictions\n",
    "    alg.fit(data_1[data_1_x_bin], data_1[target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data_1[data_1_x_bin])\n",
    "\n",
    "    row_index += 1\n",
    "\n",
    "# print and sort table\n",
    "MLA_compare.sort_values(by=['MLA Test Accuracy Mean'], ascending=False, inplace=True)\n",
    "MLA_compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='MLA Test Accuracy Mean', y='MLA Name', data=MLA_compare, color='m')\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Evaluate Model Performance"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
